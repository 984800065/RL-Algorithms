{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "980152f4",
   "metadata": {},
   "source": [
    "我目前写个一个RL算法，我的更新目标是：\n",
    "$$\n",
    "\\max\\limits{(\\mathcal{J}_{\\text{GSPO-token}}(\\theta))}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathcal{J}_{\\text{GSPO-token}}(\\theta)=\\mathbb{E}_{x\\sim\\mathcal{D}, \\{y_i\\}_{i}^{G}\\sim\\pi_{\\theta_{old}(\\cdot|x)}}\\left[\\frac{1}{G}\\sum\\limits_{i = 1}^{G}\\frac{1}{\\left|y_i \\right|}\\sum\\limits_{t=1}^{\\left| y_i \\right|}\\min{(s_{i, t}(\\theta)\\hat{A}_{i, t}, \\text{clip}(s_{i, t}(\\theta), 1 - \\epsilon, 1 + \\epsilon) \\hat{A}_{i, t})} \\right]\n",
    "$$\n",
    "其中：\n",
    "$$\n",
    "s_{i, t}(\\theta) = \\text{sg}[s_i\\theta] \\cdot \\frac{\\pi_\\theta(y_{i, t}|x, y_{i, <t})}{\\text{sg}|\\pi_{\\theta}(y_{i, t}|x, y_{i, <t}) |}\n",
    "$$\n",
    "sg表示detach。\n",
    "\n",
    "然后最后的损失函数还要对每个 minibatch 加上熵增益，你看我的代码对吗？\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "G = 32\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, action_dim),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.actor(x)\n",
    "\n",
    "\n",
    "def get_group_process_advantages(group_rewards: torch.Tensor, group_masks: torch.Tensor) -> torch.Tensor:\n",
    "    r_mean = group_rewards.mean()\n",
    "    r_std = group_rewards.std()\n",
    "    advantages_uncumsumed = (group_rewards - r_mean) / (r_std + 1e-8) * group_masks\n",
    "    advantages = torch.flip(torch.cumsum(torch.flip(advantages_uncumsumed, dims=(1,)), dim=1), dims=(1,))\n",
    "    return advantages\n",
    "\n",
    "def gspo_update(\n",
    "    model: ActorCritic,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    group_states: torch.Tensor,\n",
    "    group_actions: torch.Tensor,\n",
    "    old_group_log_probs: torch.Tensor,\n",
    "    group_advantages: torch.Tensor,\n",
    "    group_masks: torch.Tensor,\n",
    "    update_epochs: int = 4,\n",
    "    minibatch_size: int = 512,\n",
    "    beta: float = 0.01,\n",
    "    entropy_coef: float = 0.01,\n",
    "    clip_epsilon_left: float = 3e-4,\n",
    "    clip_epsilon_right: float = 4e-4\n",
    "):\n",
    "    advantages_mean = group_advantages.sum() / group_masks.sum()\n",
    "    advantages_std = torch.sqrt(((group_advantages - advantages_mean) * group_masks).pow(2).sum() / group_masks.sum())\n",
    "    group_advantages = (group_advantages - advantages_mean) / (advantages_std + 1e-8)\n",
    "\n",
    "    model.train()\n",
    "    G = group_states.shape[0]\n",
    "    for epoch in range(update_epochs):\n",
    "        # ref_group_log_probs = old_group_log_probs.clone()\n",
    "        group_log_probs = torch.zeros_like(old_group_log_probs, dtype=torch.float, device=device)\n",
    "        for g in range(G):\n",
    "            log_pi_theta_old = old_group_log_probs[g]\n",
    "            log_pi_theta_detached = group_log_probs[g]\n",
    "            with torch.no_grad():\n",
    "                for begin_idx in range(0, int(group_masks[g].sum()), minibatch_size):\n",
    "                    end_idx = begin_idx + minibatch_size\n",
    "                    batch_idx = slice(begin_idx, end_idx)\n",
    "                    \n",
    "                    probs: torch.Tensor = model(group_states[g, batch_idx]).detach()\n",
    "                    dis = torch.distributions.Categorical(probs)\n",
    "                    log_probs = dis.log_prob(group_actions[g, batch_idx]).detach()\n",
    "                    log_pi_theta_detached[batch_idx] = log_probs\n",
    "\n",
    "            importance_ratio_detached = torch.exp(1.0 / group_masks[g].sum() * (log_pi_theta_detached.sum() - log_pi_theta_old.sum())).detach()\n",
    "\n",
    "            for begin_idx in range(0, int(group_masks[g].sum()), minibatch_size):\n",
    "                end_idx = begin_idx + minibatch_size\n",
    "                batch_idx = slice(begin_idx, end_idx)\n",
    "\n",
    "                probs: torch.Tensor = model(group_states[g, batch_idx])\n",
    "                dis = torch.distributions.Categorical(probs)\n",
    "                log_probs = dis.log_prob(group_actions[g, batch_idx])\n",
    "                \n",
    "                batched_token_importance_ratio = importance_ratio_detached * torch.exp(log_probs - log_pi_theta_detached[batch_idx])\n",
    "                clip_loss = -torch.min(\n",
    "                    batched_token_importance_ratio * group_advantages[g, batch_idx], \n",
    "                    torch.clamp(\n",
    "                        batched_token_importance_ratio, \n",
    "                        1.0 - clip_epsilon_left, \n",
    "                        1.0 + clip_epsilon_right\n",
    "                    ) * group_advantages[g, batch_idx]\n",
    "                ).mean()\n",
    "\n",
    "                entropy = dis.entropy().mean()\n",
    "                entropy_bonus = -entropy_coef * entropy\n",
    "                # ref_ratio = torch.exp(ref_group_log_probs[g, batch_idx] - log_probs)\n",
    "                # KL_loss = beta * (ref_ratio - torch.log(ref_ratio) - 1).mean()\n",
    "\n",
    "                loss = clip_loss + entropy_bonus\n",
    "                loss /= G\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "def train(\n",
    "    vec_env: gym.Env, \n",
    "    model: ActorCritic, \n",
    "    state_dim: int,\n",
    "    actor_lr: float = 3e-4, \n",
    "    max_episodes: int = 100, \n",
    "    G: int = G, \n",
    "    max_steps: int = 512\n",
    "):\n",
    "    optimizer = torch.optim.AdamW([\n",
    "        {\"params\": model.actor.parameters(), \"lr\": actor_lr},\n",
    "    ])\n",
    "\n",
    "    pbar = tqdm(range(max_episodes))\n",
    "    for episode in pbar:\n",
    "        # 采样G组数据\n",
    "        model.eval()\n",
    "        group_states = torch.zeros((G, max_steps, state_dim), dtype=torch.float, device=device)\n",
    "        group_actions = torch.zeros((G, max_steps), dtype=torch.long, device=device)\n",
    "        group_log_probs = torch.zeros((G, max_steps), dtype=torch.float, device=device)\n",
    "        group_rewards = torch.zeros((G, max_steps), dtype=torch.float, device=device)\n",
    "        group_masks = torch.zeros((G, max_steps), dtype=torch.float, device=device)\n",
    "        finish_judger = np.array([False] * G)\n",
    "        with torch.no_grad():\n",
    "            states, _ = vec_env.reset()\n",
    "            for step in range(max_steps):\n",
    "                states_tensor = torch.tensor(states, dtype=torch.float).to(device)\n",
    "                probs: torch.Tensor = model(states_tensor)\n",
    "                dis = torch.distributions.Categorical(probs)\n",
    "                actions = dis.sample()\n",
    "                next_states, rewards, dones, _, _ = vec_env.step(actions.cpu().numpy().astype(np.int32))\n",
    "\n",
    "                group_states[:, step] = states_tensor\n",
    "                group_actions[:, step] = actions\n",
    "                group_log_probs[:, step] = dis.log_prob(actions)\n",
    "                group_rewards[:, step] = torch.tensor(rewards, dtype=torch.float, device=device)\n",
    "                group_masks[:, step] = torch.tensor(1 - dones, dtype=torch.float, device=device)\n",
    "                # print(dones)\n",
    "                # print(rewards)\n",
    "\n",
    "                states = next_states\n",
    "                finish_judger = finish_judger | dones\n",
    "                # print(finish_judger)\n",
    "                if all(finish_judger):\n",
    "                    break\n",
    "\n",
    "        zero_after = (1 - group_masks).cumsum(dim=1) > 0\n",
    "        group_rewards[zero_after] = 0\n",
    "        group_masks[zero_after] = 0\n",
    "        group_advantages = get_group_process_advantages(group_rewards, group_masks)\n",
    "\n",
    "        gspo_update(model, optimizer, group_states, group_actions, group_log_probs, group_advantages, group_masks)\n",
    "\n",
    "        # print(f\"Episode {episode + 1}, Reward: {avg_reward}\")\n",
    "        pbar.set_postfix(group_reward=group_rewards.sum() / G)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    vec_env = gym.make_vec(\"CartPole-v1\", num_envs=G)\n",
    "    state_dim, action_dim = vec_env.observation_space.shape[1], vec_env.action_space[0].n\n",
    "    model = ActorCritic(state_dim, action_dim).to(device)\n",
    "    train(vec_env, model, state_dim)\n",
    "```\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{J}_{\\text{EntropyBonus}} = -\\sum\\limits_{y_i}\\pi_{\\theta}(a|s)\\log{\\pi_{\\theta}(a|s)}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
